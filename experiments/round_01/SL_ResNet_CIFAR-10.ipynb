{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e0127f-39ce-4528-b887-4899de0778cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "---------SL ResNet18 on CIFAR-10 - Epoch 20----------\n",
      "We use 4 GPUs\n",
      "DataParallel(\n",
      "  (module): ResNet18_client_side(\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "We use 4 GPUs\n",
      "DataParallel(\n",
      "  (module): ResNet18_server_side(\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Baseblock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dim_change): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Baseblock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer5): Sequential(\n",
      "      (0): Baseblock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dim_change): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Baseblock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer6): Sequential(\n",
      "      (0): Baseblock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dim_change): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Baseblock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (averagePool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 39.575 \tLoss: 1.7066\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 9.443 \tLoss: 3.7603\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 53.725 \tLoss: 1.4552\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 28.770 \tLoss: 2.9008\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 49.980 \tLoss: 1.5984\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 23.086 \tLoss: 2.9941\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 43.725 \tLoss: 1.6808\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 28.789 \tLoss: 2.3901\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 48.933 \tLoss: 1.4827\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 32.207 \tLoss: 2.3456\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   0, Avg Accuracy 47.188 | Avg Loss 1.585\n",
      " Test: Round   0, Avg Accuracy 24.459 | Avg Loss 2.878\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 62.363 \tLoss: 1.1455\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 32.549 \tLoss: 3.0730\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 56.480 \tLoss: 1.4496\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 28.936 \tLoss: 2.6703\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 52.113 \tLoss: 1.4457\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 34.268 \tLoss: 2.3224\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 52.124 \tLoss: 1.5154\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 29.414 \tLoss: 2.1557\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 54.061 \tLoss: 1.3019\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 39.141 \tLoss: 2.2824\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   1, Avg Accuracy 55.428 | Avg Loss 1.372\n",
      " Test: Round   1, Avg Accuracy 32.861 | Avg Loss 2.501\n",
      "==========================================================\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 59.875 \tLoss: 1.3424\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 33.350 \tLoss: 2.2255\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 56.013 \tLoss: 1.3281\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 39.678 \tLoss: 1.7786\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 65.501 \tLoss: 1.0386\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 37.510 \tLoss: 2.6504\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 59.112 \tLoss: 1.2376\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 42.715 \tLoss: 1.7655\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 57.860 \tLoss: 1.2023\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 39.209 \tLoss: 2.1670\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   2, Avg Accuracy 59.672 | Avg Loss 1.230\n",
      " Test: Round   2, Avg Accuracy 38.492 | Avg Loss 2.117\n",
      "==========================================================\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 62.134 \tLoss: 1.3188\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 36.357 \tLoss: 2.0616\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 58.270 \tLoss: 1.2452\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 40.889 \tLoss: 1.7483\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 67.470 \tLoss: 0.9653\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 36.084 \tLoss: 2.7091\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 62.084 \tLoss: 1.1480\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 45.059 \tLoss: 1.7665\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 60.304 \tLoss: 1.1433\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 44.365 \tLoss: 2.1030\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   3, Avg Accuracy 62.052 | Avg Loss 1.164\n",
      " Test: Round   3, Avg Accuracy 40.551 | Avg Loss 2.078\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 69.720 \tLoss: 0.9092\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 38.105 \tLoss: 2.7085\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 65.802 \tLoss: 1.1796\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 37.031 \tLoss: 2.2573\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 58.744 \tLoss: 1.2349\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 44.854 \tLoss: 2.3477\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 62.851 \tLoss: 1.1103\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 44.111 \tLoss: 1.5967\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 64.711 \tLoss: 1.0709\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 45.498 \tLoss: 1.8212\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   4, Avg Accuracy 64.365 | Avg Loss 1.101\n",
      " Test: Round   4, Avg Accuracy 41.920 | Avg Loss 2.146\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 72.233 \tLoss: 0.8149\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 40.078 \tLoss: 2.7604\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 66.255 \tLoss: 1.0309\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 48.984 \tLoss: 1.7065\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 64.052 \tLoss: 1.0460\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 48.779 \tLoss: 1.8050\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 65.529 \tLoss: 1.0579\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 46.914 \tLoss: 1.5182\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 70.026 \tLoss: 0.9695\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 42.109 \tLoss: 1.8312\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   5, Avg Accuracy 67.619 | Avg Loss 0.984\n",
      " Test: Round   5, Avg Accuracy 45.373 | Avg Loss 1.924\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 64.709 \tLoss: 1.0364\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 51.221 \tLoss: 1.4796\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 71.861 \tLoss: 0.8436\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 40.205 \tLoss: 2.4988\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 64.577 \tLoss: 1.0693\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 48.477 \tLoss: 1.8670\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 69.011 \tLoss: 0.9275\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 50.117 \tLoss: 1.6955\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 69.632 \tLoss: 1.0241\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 39.053 \tLoss: 2.0525\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   6, Avg Accuracy 67.958 | Avg Loss 0.980\n",
      " Test: Round   6, Avg Accuracy 45.814 | Avg Loss 1.919\n",
      "==========================================================\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 70.399 \tLoss: 0.8879\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 48.555 \tLoss: 1.7721\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 74.830 \tLoss: 0.7398\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 40.137 \tLoss: 2.6874\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 61.706 \tLoss: 1.2549\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 50.225 \tLoss: 1.4345\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 72.129 \tLoss: 0.8539\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 45.371 \tLoss: 1.8777\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 64.932 \tLoss: 1.0358\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 51.328 \tLoss: 1.8441\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   7, Avg Accuracy 68.799 | Avg Loss 0.954\n",
      " Test: Round   7, Avg Accuracy 47.123 | Avg Loss 1.923\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 75.629 \tLoss: 0.7205\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 42.842 \tLoss: 2.6350\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 71.009 \tLoss: 0.9189\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 41.650 \tLoss: 2.0896\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 65.408 \tLoss: 1.1139\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 52.607 \tLoss: 1.3590\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 69.950 \tLoss: 0.9168\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 52.178 \tLoss: 1.5840\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 68.156 \tLoss: 0.9190\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 52.314 \tLoss: 1.6723\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   8, Avg Accuracy 70.030 | Avg Loss 0.918\n",
      " Test: Round   8, Avg Accuracy 48.318 | Avg Loss 1.868\n",
      "==========================================================\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 70.120 \tLoss: 0.8285\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 54.131 \tLoss: 2.0035\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 68.216 \tLoss: 1.1721\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 45.352 \tLoss: 1.7908\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 71.328 \tLoss: 0.8841\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 51.855 \tLoss: 1.7767\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 69.386 \tLoss: 0.9119\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 50.537 \tLoss: 1.5389\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 75.397 \tLoss: 0.7160\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 42.471 \tLoss: 2.6648\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   9, Avg Accuracy 70.889 | Avg Loss 0.902\n",
      " Test: Round   9, Avg Accuracy 48.869 | Avg Loss 1.955\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 79.188 \tLoss: 0.5834\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 42.666 \tLoss: 3.2579\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 71.758 \tLoss: 0.9913\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 46.270 \tLoss: 2.0246\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 66.466 \tLoss: 1.0551\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 56.553 \tLoss: 1.5128\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 73.228 \tLoss: 0.8031\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 54.404 \tLoss: 1.6022\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 70.219 \tLoss: 0.9075\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 55.264 \tLoss: 1.3378\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  10, Avg Accuracy 72.172 | Avg Loss 0.868\n",
      " Test: Round  10, Avg Accuracy 51.031 | Avg Loss 1.947\n",
      "==========================================================\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 73.829 \tLoss: 0.7857\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 53.145 \tLoss: 1.5584\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 74.677 \tLoss: 0.8208\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 47.207 \tLoss: 1.8614\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 68.470 \tLoss: 0.9482\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 52.090 \tLoss: 1.7205\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 71.446 \tLoss: 0.8588\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 54.795 \tLoss: 1.3084\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 77.784 \tLoss: 0.6605\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 43.564 \tLoss: 2.2685\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  11, Avg Accuracy 73.241 | Avg Loss 0.815\n",
      " Test: Round  11, Avg Accuracy 50.160 | Avg Loss 1.743\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 80.576 \tLoss: 0.5455\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 44.531 \tLoss: 3.2769\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 66.758 \tLoss: 1.1268\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 55.176 \tLoss: 1.2433\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 74.993 \tLoss: 0.7815\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 51.572 \tLoss: 1.7279\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 73.771 \tLoss: 0.8261\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 51.973 \tLoss: 1.6316\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 70.994 \tLoss: 0.8793\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 53.154 \tLoss: 1.6358\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  12, Avg Accuracy 73.419 | Avg Loss 0.832\n",
      " Test: Round  12, Avg Accuracy 51.281 | Avg Loss 1.903\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.525 \tLoss: 0.8275\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 56.162 \tLoss: 1.4035\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 75.012 \tLoss: 0.7685\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 52.080 \tLoss: 1.6544\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 70.257 \tLoss: 0.8980\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 56.357 \tLoss: 1.5938\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 75.733 \tLoss: 0.7546\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 56.992 \tLoss: 1.5094\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 78.944 \tLoss: 0.6187\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.254 \tLoss: 2.1955\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  13, Avg Accuracy 74.494 | Avg Loss 0.773\n",
      " Test: Round  13, Avg Accuracy 53.369 | Avg Loss 1.671\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 70.147 \tLoss: 0.9493\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 57.314 \tLoss: 1.1722\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 79.772 \tLoss: 0.5751\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 43.467 \tLoss: 2.9870\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 71.352 \tLoss: 0.8792\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 54.170 \tLoss: 1.5065\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 74.022 \tLoss: 0.9031\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 53.252 \tLoss: 1.7149\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 75.419 \tLoss: 0.7612\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 53.105 \tLoss: 1.7104\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  14, Avg Accuracy 74.142 | Avg Loss 0.814\n",
      " Test: Round  14, Avg Accuracy 52.262 | Avg Loss 1.818\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 73.501 \tLoss: 0.8088\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 54.170 \tLoss: 1.3146\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 79.920 \tLoss: 0.5899\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 46.074 \tLoss: 2.4934\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 75.453 \tLoss: 0.7952\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 47.861 \tLoss: 1.9067\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 76.298 \tLoss: 0.7279\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 48.984 \tLoss: 1.8409\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 73.423 \tLoss: 0.8191\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 57.617 \tLoss: 1.4375\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  15, Avg Accuracy 75.719 | Avg Loss 0.748\n",
      " Test: Round  15, Avg Accuracy 50.941 | Avg Loss 1.799\n",
      "==========================================================\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 78.673 \tLoss: 0.6528\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 57.129 \tLoss: 1.8432\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 75.723 \tLoss: 0.8236\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 47.480 \tLoss: 1.8197\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.706 \tLoss: 0.8584\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 58.887 \tLoss: 1.2101\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 73.307 \tLoss: 0.7899\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 61.289 \tLoss: 1.4050\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 80.508 \tLoss: 0.5845\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.908 \tLoss: 2.1424\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  16, Avg Accuracy 76.183 | Avg Loss 0.742\n",
      " Test: Round  16, Avg Accuracy 54.139 | Avg Loss 1.684\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 71.963 \tLoss: 0.9099\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 59.131 \tLoss: 1.1428\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 77.122 \tLoss: 0.7717\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 54.180 \tLoss: 1.6367\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 76.685 \tLoss: 0.7326\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 54.082 \tLoss: 1.6809\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 73.954 \tLoss: 0.8077\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 54.365 \tLoss: 1.7032\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 81.412 \tLoss: 0.5649\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.166 \tLoss: 2.4320\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  17, Avg Accuracy 76.227 | Avg Loss 0.757\n",
      " Test: Round  17, Avg Accuracy 53.385 | Avg Loss 1.719\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.137 \tLoss: 0.8989\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 60.986 \tLoss: 1.1247\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 77.701 \tLoss: 0.7189\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 52.520 \tLoss: 1.6578\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 77.071 \tLoss: 0.7122\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 54.521 \tLoss: 1.8244\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 74.457 \tLoss: 0.7626\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 57.539 \tLoss: 1.4124\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 81.128 \tLoss: 0.5558\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.820 \tLoss: 2.4981\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  18, Avg Accuracy 76.499 | Avg Loss 0.730\n",
      " Test: Round  18, Avg Accuracy 54.277 | Avg Loss 1.703\n",
      "==========================================================\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 76.815 \tLoss: 0.8105\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 47.969 \tLoss: 1.8833\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.816 \tLoss: 0.8641\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 60.762 \tLoss: 1.2651\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 77.151 \tLoss: 0.7003\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 60.605 \tLoss: 1.3477\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 81.044 \tLoss: 0.5514\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 47.217 \tLoss: 2.3239\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 74.289 \tLoss: 0.7933\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 55.439 \tLoss: 1.5299\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  19, Avg Accuracy 76.423 | Avg Loss 0.744\n",
      " Test: Round  19, Avg Accuracy 54.398 | Avg Loss 1.670\n",
      "==========================================================\n",
      "Training and Evaluation completed!\n",
      "Saved: SL ResNet18 on CIFAR-10 - Epoch 20.xlsx and SL ResNet18 on CIFAR-10 - Epoch 20.csv\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================\n",
    "# Split learning: ResNet18 on HAM10000\n",
    "# HAM10000 dataset: Tschandl, P.: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions (2018), doi:10.7910/DVN/DBW86T\n",
    "\n",
    "# We have three versions of our implementations\n",
    "# Version1: without using socket and no DP+PixelDP\n",
    "# Version2: with using socket but no DP+PixelDP\n",
    "# Version3: without using socket but with DP+PixelDP\n",
    "\n",
    "# This program is Version1: Single program simulation \n",
    "# ============================================================================\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from glob import glob \n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def _sync_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(torch.cuda.get_device_name(0))    \n",
    "\n",
    "#===================================================================  \n",
    "program = \"SL ResNet18 on CIFAR-10 - Epoch 20\"\n",
    "print(f\"---------{program}----------\")              # this is to identify the program in the slurm outputs files\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# To print in color -------test/train of the client side\n",
    "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \n",
    "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))     \n",
    "\n",
    "#===================================================================  \n",
    "# No. of users\n",
    "num_users = 5\n",
    "epochs = 20\n",
    "frac = 1   # participation of clients; if 1 then 100% clients participate in SL\n",
    "lr = 0.0001\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Client-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at client side\n",
    "class ResNet18_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18_client_side, self).__init__()\n",
    "        self.layer1 = nn.Sequential (\n",
    "                nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU (inplace = True),\n",
    "                nn.MaxPool2d(kernel_size = 3, stride = 2, padding =1),\n",
    "            )\n",
    "        self.layer2 = nn.Sequential  (\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU (inplace = True),\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.BatchNorm2d(64),              \n",
    "            )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        resudial1 = F.relu(self.layer1(x))\n",
    "        out1 = self.layer2(resudial1)\n",
    "        out1 = out1 + resudial1 # adding the resudial inputs -- downsampling not required in this layer\n",
    "        resudial2 = F.relu(out1)\n",
    "        return resudial2\n",
    " \n",
    " \n",
    "           \n",
    "\n",
    "net_glob_client = ResNet18_client_side()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_client = nn.DataParallel(net_glob_client)   # to use the multiple GPUs; later we can change this to CPUs only \n",
    "\n",
    "net_glob_client.to(device)\n",
    "print(net_glob_client)     \n",
    "\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Server-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at server side\n",
    "class Baseblock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, input_planes, planes, stride = 1, dim_change = None):\n",
    "        super(Baseblock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_planes, planes, stride =  stride, kernel_size = 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, stride = 1, kernel_size = 3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        \n",
    "        if self.dim_change is not None:\n",
    "            res =self.dim_change(res)\n",
    "            \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ResNet18_server_side(nn.Module):\n",
    "    def __init__(self, block, num_layers, classes):\n",
    "        super(ResNet18_server_side, self).__init__()\n",
    "        self.input_planes = 64\n",
    "        self.layer3 = nn.Sequential (\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU (inplace = True),\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.BatchNorm2d(64),       \n",
    "                )   \n",
    "        \n",
    "        self.layer4 = self._layer(block, 128, num_layers[0], stride = 2)\n",
    "        self.layer5 = self._layer(block, 256, num_layers[1], stride = 2)\n",
    "        self.layer6 = self._layer(block, 512, num_layers[2], stride = 2)\n",
    "        self. averagePool = nn.AvgPool2d(kernel_size = 7, stride = 1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "    def _layer(self, block, planes, num_layers, stride = 2):\n",
    "        dim_change = None\n",
    "        if stride != 1 or planes != self.input_planes * block.expansion:\n",
    "            dim_change = nn.Sequential(nn.Conv2d(self.input_planes, planes*block.expansion, kernel_size = 1, stride = stride),\n",
    "                                       nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers = []\n",
    "        netLayers.append(block(self.input_planes, planes, stride = stride, dim_change = dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1, num_layers):\n",
    "            netLayers.append(block(self.input_planes, planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "            \n",
    "        return nn.Sequential(*netLayers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out2 = self.layer3(x)\n",
    "        out2 = out2 + x          # adding the resudial inputs -- downsampling not required in this layer\n",
    "        x3 = F.relu(out2)\n",
    "        \n",
    "        x4 = self. layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        x6 = self.layer6(x5)\n",
    "        \n",
    "        # x7 = F.avg_pool2d(x6, 7)\n",
    "        x7 = F.adaptive_avg_pool2d(x6, 1) \n",
    "        x8 = x7.view(x7.size(0), -1) \n",
    "        y_hat =self.fc(x8)\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "# net_glob_server = ResNet18_server_side(Baseblock, [2,2,2], 7) #7 is my numbr of classes\n",
    "net_glob_server = ResNet18_server_side(Baseblock, [2,2,2], 10)  # 10 classes for CIFAR-10\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_server = nn.DataParallel(net_glob_server)   # to use the multiple GPUs \n",
    "\n",
    "net_glob_server.to(device)\n",
    "print(net_glob_server)      \n",
    "\n",
    "#===================================================================================\n",
    "# For Server Side Loss and Accuracy \n",
    "loss_train_collect = []\n",
    "acc_train_collect = []\n",
    "loss_test_collect = []\n",
    "acc_test_collect = []\n",
    "batch_acc_train = []\n",
    "batch_loss_train = []\n",
    "batch_acc_test = []\n",
    "batch_loss_test = []\n",
    "round_time_collect = [] # Stores seconds per global round\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "#====================================================================================================\n",
    "#                                  Server Side Program\n",
    "#====================================================================================================\n",
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = 100.00 *correct.float()/preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "# to print train - test together in each round-- these are made global\n",
    "acc_avg_all_user_train = 0\n",
    "loss_avg_all_user_train = 0\n",
    "loss_train_collect_user = []\n",
    "acc_train_collect_user = []\n",
    "loss_test_collect_user = []\n",
    "acc_test_collect_user = []\n",
    "\n",
    "\n",
    "#client idx collector\n",
    "idx_collect = []\n",
    "l_epoch_check = False\n",
    "fed_check = False\n",
    "\n",
    "# Server-side function associated with Training \n",
    "def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch):\n",
    "    global net_glob_server, criterion, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n",
    "    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect\n",
    "    global loss_train_collect_user, acc_train_collect_user\n",
    "    \n",
    "    net_glob_server.train()\n",
    "    optimizer_server = torch.optim.Adam(net_glob_server.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    # train and update\n",
    "    optimizer_server.zero_grad()\n",
    "    \n",
    "    fx_client = fx_client.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    #---------forward prop-------------\n",
    "    fx_server = net_glob_server(fx_client)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(fx_server, y)\n",
    "    # calculate accuracy\n",
    "    acc = calculate_accuracy(fx_server, y)\n",
    "    \n",
    "    #--------backward prop--------------\n",
    "    loss.backward()\n",
    "    dfx_client = fx_client.grad.clone().detach()\n",
    "    optimizer_server.step()\n",
    "    \n",
    "    batch_loss_train.append(loss.item())\n",
    "    batch_acc_train.append(acc.item())\n",
    "    \n",
    "    # server-side model net_glob_server is global so it is updated automatically in each pass to this function\n",
    "        # count1: to track the completion of the local batch associated with one client\n",
    "    count1 += 1\n",
    "    if count1 == len_batch:\n",
    "        acc_avg_train = sum(batch_acc_train)/len(batch_acc_train)           # it has accuracy for one batch\n",
    "        loss_avg_train = sum(batch_loss_train)/len(batch_loss_train)\n",
    "        \n",
    "        batch_acc_train = []\n",
    "        batch_loss_train = []\n",
    "        count1 = 0\n",
    "        \n",
    "        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, l_epoch_count, acc_avg_train, loss_avg_train))\n",
    "        \n",
    "                \n",
    "        # If one local epoch is completed, after this a new client will come\n",
    "        if l_epoch_count == l_epoch-1:\n",
    "            \n",
    "            l_epoch_check = True                # for evaluate_server function - to check local epoch has hitted \n",
    "                       \n",
    "            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n",
    "            # this is because we work on the last trained model and its accuracy (not earlier cases)\n",
    "            \n",
    "            #print(\"accuracy = \", acc_avg_train)\n",
    "            acc_avg_train_all = acc_avg_train\n",
    "            loss_avg_train_all = loss_avg_train\n",
    "                        \n",
    "            # accumulate accuracy and loss for each new user\n",
    "            loss_train_collect_user.append(loss_avg_train_all)\n",
    "            acc_train_collect_user.append(acc_avg_train_all)\n",
    "            \n",
    "            # collect the id of each new user                        \n",
    "            if idx not in idx_collect:\n",
    "                idx_collect.append(idx) \n",
    "                #print(idx_collect)\n",
    "        \n",
    "        # This is to check if all users are served for one round --------------------\n",
    "        if len(idx_collect) == num_users:\n",
    "            fed_check = True                                                  # for evaluate_server function  - to check fed check has hitted\n",
    "            # all users served for one round ------------------------- output print and update is done in evaluate_server()\n",
    "            # for nicer display \n",
    "                        \n",
    "            idx_collect = []\n",
    "            \n",
    "            acc_avg_all_user_train = sum(acc_train_collect_user)/len(acc_train_collect_user)\n",
    "            loss_avg_all_user_train = sum(loss_train_collect_user)/len(loss_train_collect_user)\n",
    "            \n",
    "            loss_train_collect.append(loss_avg_all_user_train)\n",
    "            acc_train_collect.append(acc_avg_all_user_train)\n",
    "            \n",
    "            acc_train_collect_user = []\n",
    "            loss_train_collect_user = []\n",
    "            \n",
    "    # send gradients to the client               \n",
    "    return dfx_client\n",
    "\n",
    "# Server-side functions associated with Testing\n",
    "def evaluate_server(fx_client, y, idx, len_batch, ell):\n",
    "    global net_glob_server, criterion, batch_acc_test, batch_loss_test\n",
    "    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, l_epoch_check, fed_check\n",
    "    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n",
    "    \n",
    "    net_glob_server.eval()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        fx_client = fx_client.to(device)\n",
    "        y = y.to(device) \n",
    "        #---------forward prop-------------\n",
    "        fx_server = net_glob_server(fx_client)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(fx_server, y)\n",
    "        # calculate accuracy\n",
    "        acc = calculate_accuracy(fx_server, y)\n",
    "        \n",
    "        \n",
    "        batch_loss_test.append(loss.item())\n",
    "        batch_acc_test.append(acc.item())\n",
    "        \n",
    "               \n",
    "        count2 += 1\n",
    "        if count2 == len_batch:\n",
    "            acc_avg_test = sum(batch_acc_test)/len(batch_acc_test)\n",
    "            loss_avg_test = sum(batch_loss_test)/len(batch_loss_test)\n",
    "            \n",
    "            batch_acc_test = []\n",
    "            batch_loss_test = []\n",
    "            count2 = 0\n",
    "            \n",
    "            prGreen('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, acc_avg_test, loss_avg_test))\n",
    "            \n",
    "            # if a local epoch is completed   \n",
    "            if l_epoch_check:\n",
    "                l_epoch_check = False\n",
    "                \n",
    "                # Store the last accuracy and loss\n",
    "                acc_avg_test_all = acc_avg_test\n",
    "                loss_avg_test_all = loss_avg_test\n",
    "                        \n",
    "                loss_test_collect_user.append(loss_avg_test_all)\n",
    "                acc_test_collect_user.append(acc_avg_test_all)\n",
    "                \n",
    "            # if all users are served for one round ----------                    \n",
    "            if fed_check:\n",
    "                fed_check = False\n",
    "                                \n",
    "                acc_avg_all_user = sum(acc_test_collect_user)/len(acc_test_collect_user)\n",
    "                loss_avg_all_user = sum(loss_test_collect_user)/len(loss_test_collect_user)\n",
    "            \n",
    "                loss_test_collect.append(loss_avg_all_user)\n",
    "                acc_test_collect.append(acc_avg_all_user)\n",
    "                acc_test_collect_user = []\n",
    "                loss_test_collect_user= []\n",
    "                              \n",
    "                print(\"====================== SERVER V1==========================\")\n",
    "                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train, loss_avg_all_user_train))\n",
    "                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user, loss_avg_all_user))\n",
    "                print(\"==========================================================\")\n",
    "         \n",
    "    return \n",
    "\n",
    "#==============================================================================================================\n",
    "#                                       Clients Side Program\n",
    "#==============================================================================================================\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "# Client-side functions associated with Training and Testing\n",
    "class Client(object):\n",
    "    def __init__(self, net_client_model, idx, lr, device, dataset_train = None, dataset_test = None, idxs = None, idxs_test = None):\n",
    "        self.idx = idx\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.local_ep = 1 \n",
    "        #self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = 128, shuffle = True)\n",
    "        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size = 128, shuffle = True)\n",
    "        \n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr = self.lr) \n",
    "        \n",
    "        for iter in range(self.local_ep):\n",
    "            len_batch = len(self.ldr_train)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer_client.zero_grad()\n",
    "                #---------forward prop-------------\n",
    "                fx = net(images)\n",
    "                client_fx = fx.clone().detach().requires_grad_(True)\n",
    "                \n",
    "                # Sending activations to server and receiving gradients from server\n",
    "                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n",
    "                \n",
    "                #--------backward prop -------------\n",
    "                fx.backward(dfx)\n",
    "                optimizer_client.step()\n",
    "                            \n",
    "            \n",
    "            #prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n",
    "           \n",
    "        return net.state_dict() \n",
    "    \n",
    "    def evaluate(self, net, ell):\n",
    "        net.eval()\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            len_batch = len(self.ldr_test)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                #---------forward prop-------------\n",
    "                fx = net(images)\n",
    "                \n",
    "                # Sending activations to server \n",
    "                evaluate_server(fx, labels, self.idx, len_batch, ell)\n",
    "            \n",
    "            #prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n",
    "            \n",
    "        return          \n",
    "#=====================================================================================================\n",
    "# dataset_iid() will create a dictionary to collect the indices of the data samples randomly for each client\n",
    "# IID HAM10000 datasets will be created based on this\n",
    "def dataset_iid(dataset, num_users):\n",
    "    \n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users    \n",
    "\n",
    "def dataset_noniid_dirichlet(dataset, num_users, alpha=0.5, seed=SEED):\n",
    "    \"\"\"Return dict_users: {client_id -> set(sample_indices)} using Dirichlet(α).\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    labels = np.array(dataset.targets)  # CIFAR-10 stores labels here\n",
    "    n_classes = len(np.unique(labels))\n",
    "    dict_users = {i: set() for i in range(num_users)}\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        rng.shuffle(idxs)\n",
    "        # draw proportions for this class\n",
    "        p = rng.dirichlet([alpha] * num_users)\n",
    "        # split indices by cumulative proportions\n",
    "        cuts = (np.cumsum(p) * len(idxs)).astype(int)[:-1]\n",
    "        splits = np.split(idxs, cuts)\n",
    "        for client_id, split in enumerate(splits):\n",
    "            dict_users[client_id].update(split.tolist())\n",
    "    return dict_users\n",
    "\n",
    "            \n",
    "#=============================================================================\n",
    "#                         Data loading \n",
    "#============================================================================= \n",
    "\n",
    "\n",
    "\n",
    "# CIFAR-10 normalization\n",
    "cifar_mean = [0.4914, 0.4822, 0.4465]\n",
    "cifar_std  = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
    "])\n",
    "\n",
    "dataset_train = datasets.CIFAR10(root=\"data\", train=True,  download=True, transform=train_transforms)\n",
    "dataset_test  = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transforms)\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# dict_users = dataset_iid(dataset_train, num_users)\n",
    "# dict_users_test = dataset_iid(dataset_test, num_users)\n",
    "\n",
    "ALPHA = 0.5   # ← your requested alpha\n",
    "\n",
    "dict_users      = dataset_noniid_dirichlet(dataset_train, num_users, alpha=ALPHA)\n",
    "dict_users_test = dataset_iid(dataset_test, num_users)  \n",
    "\n",
    "\n",
    "\n",
    "#net_glob_client.train()\n",
    "# this epoch is global epoch, also known as rounds\n",
    "# this epoch is global epoch, also known as rounds\n",
    "for iter in range(epochs):\n",
    "    # ---- START TIMER ----\n",
    "    _sync_cuda()\n",
    "    t0 = time.perf_counter()\n",
    "    # ---------------------\n",
    "\n",
    "    m = max(int(frac * num_users), 1)\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace=False)\n",
    "\n",
    "    # Sequential training/testing among clients\n",
    "    for idx in idxs_users:\n",
    "        local = Client(net_glob_client, idx, lr, device,\n",
    "                       dataset_train=dataset_train, dataset_test=dataset_test,\n",
    "                       idxs=dict_users[idx], idxs_test=dict_users_test[idx])\n",
    "\n",
    "        # Training\n",
    "        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n",
    "\n",
    "        # Testing\n",
    "        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter)\n",
    "\n",
    "        # Update client weights for next client\n",
    "         # copy weight to net_glob_client -- use to update the client-side model of the next client to be trained\n",
    "        net_glob_client.load_state_dict(w_client)\n",
    "\n",
    "    # ---- STOP TIMER & LOG ----\n",
    "    _sync_cuda()\n",
    "    dt = time.perf_counter() - t0\n",
    "    round_time_collect.append(dt)\n",
    "    # -------------------------- \n",
    "\n",
    "print(\"Training and Evaluation completed!\")    \n",
    "\n",
    "#===============================================================================\n",
    "# Save output data to .excel file (we use for comparision plots)\n",
    "# Align lengths safely (in case of early stop or mismatch)\n",
    "n = min(len(acc_train_collect), len(acc_test_collect), len(round_time_collect))\n",
    "round_process = list(range(1, n+1))\n",
    "\n",
    "df = DataFrame({\n",
    "    'round': round_process,\n",
    "    'acc_train': acc_train_collect[:n],\n",
    "    'acc_test':  acc_test_collect[:n],\n",
    "    'round_time_sec': round_time_collect[:n],   # time\n",
    "})\n",
    "\n",
    "file_name_xlsx = program + \".xlsx\"\n",
    "df.to_excel(file_name_xlsx, sheet_name=\"SL_test\", index=False)\n",
    "\n",
    "# write CSV — easier for scripting/comparison tools\n",
    "file_name_csv = program + \".csv\"\n",
    "df.to_csv(file_name_csv, index=False)\n",
    "\n",
    "print(\"Saved:\", file_name_xlsx, \"and\", file_name_csv)\n",
    "\n",
    "#=============================================================================\n",
    "#                         Program Completed\n",
    "#============================================================================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8c5b5-fbc2-4e74-8f5d-4ffffb39f044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
