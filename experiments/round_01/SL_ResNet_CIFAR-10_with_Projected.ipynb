{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e0127f-39ce-4528-b887-4899de0778cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "---------SL ResNet18 on CIFAR-10 - with Projection----------\n",
      "We use 4 GPUs\n",
      "DataParallel(\n",
      "  (module): ResNet18_client_side(\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "We use 4 GPUs\n",
      "DataParallel(\n",
      "  (module): ResNet18_server_side(\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Baseblock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dim_change): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Baseblock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer5): Sequential(\n",
      "      (0): Baseblock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dim_change): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Baseblock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer6): Sequential(\n",
      "      (0): Baseblock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dim_change): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Baseblock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (averagePool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 39.575 \tLoss: 1.7066\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 9.443 \tLoss: 3.7603\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 53.062 \tLoss: 1.4627\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 29.229 \tLoss: 2.8897\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 48.745 \tLoss: 1.6194\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 23.174 \tLoss: 2.9620\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 44.435 \tLoss: 1.6703\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 26.455 \tLoss: 2.5003\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 48.443 \tLoss: 1.4981\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 29.326 \tLoss: 2.4957\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   0, Avg Accuracy 46.852 | Avg Loss 1.591\n",
      " Test: Round   0, Avg Accuracy 23.525 | Avg Loss 2.922\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 61.909 \tLoss: 1.1689\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 32.207 \tLoss: 3.0713\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 56.012 \tLoss: 1.4680\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 29.492 \tLoss: 2.7284\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 51.354 \tLoss: 1.4584\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 34.580 \tLoss: 2.2987\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 51.468 \tLoss: 1.5176\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 30.156 \tLoss: 2.1500\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 53.036 \tLoss: 1.3271\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 38.477 \tLoss: 2.3098\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   1, Avg Accuracy 54.756 | Avg Loss 1.388\n",
      " Test: Round   1, Avg Accuracy 32.982 | Avg Loss 2.512\n",
      "==========================================================\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 58.385 \tLoss: 1.3626\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 31.855 \tLoss: 2.2936\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 54.366 \tLoss: 1.3583\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 37.188 \tLoss: 1.8390\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 64.924 \tLoss: 1.0619\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 36.328 \tLoss: 2.7328\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 59.157 \tLoss: 1.2521\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 42.461 \tLoss: 1.8148\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 57.006 \tLoss: 1.2263\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 40.264 \tLoss: 2.1725\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   2, Avg Accuracy 58.767 | Avg Loss 1.252\n",
      " Test: Round   2, Avg Accuracy 37.619 | Avg Loss 2.171\n",
      "==========================================================\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 61.296 \tLoss: 1.3634\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 35.156 \tLoss: 2.0842\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 58.222 \tLoss: 1.2688\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 38.184 \tLoss: 1.8305\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 66.424 \tLoss: 0.9795\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 37.090 \tLoss: 2.7467\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 62.200 \tLoss: 1.1633\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 46.318 \tLoss: 1.7412\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 59.558 \tLoss: 1.1669\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 43.623 \tLoss: 2.1127\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   3, Avg Accuracy 61.540 | Avg Loss 1.188\n",
      " Test: Round   3, Avg Accuracy 40.074 | Avg Loss 2.103\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 69.938 \tLoss: 0.9293\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 38.711 \tLoss: 2.7020\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 64.619 \tLoss: 1.1951\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 34.463 \tLoss: 2.2612\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 58.595 \tLoss: 1.2533\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 44.844 \tLoss: 2.3213\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 62.634 \tLoss: 1.1283\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 41.963 \tLoss: 1.6315\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 63.597 \tLoss: 1.1020\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 43.486 \tLoss: 1.8272\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   4, Avg Accuracy 63.876 | Avg Loss 1.122\n",
      " Test: Round   4, Avg Accuracy 40.693 | Avg Loss 2.149\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 71.865 \tLoss: 0.8289\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 39.297 \tLoss: 2.7893\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 65.788 \tLoss: 1.0573\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 48.457 \tLoss: 1.6715\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 62.929 \tLoss: 1.0736\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 47.344 \tLoss: 1.8756\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 64.258 \tLoss: 1.0835\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 46.934 \tLoss: 1.5088\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 69.473 \tLoss: 0.9875\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 42.910 \tLoss: 1.8578\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   5, Avg Accuracy 66.863 | Avg Loss 1.006\n",
      " Test: Round   5, Avg Accuracy 44.988 | Avg Loss 1.941\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 64.033 \tLoss: 1.0501\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 49.443 \tLoss: 1.4917\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 71.312 \tLoss: 0.8505\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 39.248 \tLoss: 2.4735\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 63.732 \tLoss: 1.0903\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 45.850 \tLoss: 1.9355\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 68.590 \tLoss: 0.9452\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 49.453 \tLoss: 1.7609\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 69.134 \tLoss: 1.0464\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 36.025 \tLoss: 2.1402\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   6, Avg Accuracy 67.360 | Avg Loss 0.996\n",
      " Test: Round   6, Avg Accuracy 44.004 | Avg Loss 1.960\n",
      "==========================================================\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 69.353 \tLoss: 0.9030\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 46.670 \tLoss: 1.8238\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 74.407 \tLoss: 0.7630\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 39.844 \tLoss: 2.6730\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 61.277 \tLoss: 1.2743\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 45.615 \tLoss: 1.4928\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 72.097 \tLoss: 0.8554\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 45.410 \tLoss: 1.8980\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 63.218 \tLoss: 1.0726\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 52.461 \tLoss: 1.8265\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   7, Avg Accuracy 68.070 | Avg Loss 0.974\n",
      " Test: Round   7, Avg Accuracy 46.000 | Avg Loss 1.943\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 75.324 \tLoss: 0.7439\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 41.230 \tLoss: 2.5516\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 70.461 \tLoss: 0.9151\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 40.098 \tLoss: 2.2065\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 65.464 \tLoss: 1.1377\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 51.992 \tLoss: 1.4187\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 69.106 \tLoss: 0.9392\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 53.359 \tLoss: 1.5349\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 67.243 \tLoss: 0.9486\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 51.904 \tLoss: 1.7261\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   8, Avg Accuracy 69.520 | Avg Loss 0.937\n",
      " Test: Round   8, Avg Accuracy 47.717 | Avg Loss 1.888\n",
      "==========================================================\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 69.705 \tLoss: 0.8396\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 50.332 \tLoss: 2.1069\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 67.710 \tLoss: 1.1908\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 43.193 \tLoss: 1.9912\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 70.852 \tLoss: 0.9010\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 51.035 \tLoss: 1.7587\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 69.227 \tLoss: 0.9317\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 51.016 \tLoss: 1.4900\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 75.215 \tLoss: 0.7325\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 41.025 \tLoss: 2.6455\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round   9, Avg Accuracy 70.542 | Avg Loss 0.919\n",
      " Test: Round   9, Avg Accuracy 47.320 | Avg Loss 1.998\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 78.723 \tLoss: 0.6004\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 43.223 \tLoss: 3.1899\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 71.254 \tLoss: 0.9946\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 41.953 \tLoss: 2.2136\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 65.542 \tLoss: 1.0812\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 54.727 \tLoss: 1.6628\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 72.792 \tLoss: 0.8294\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 53.564 \tLoss: 1.6809\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 69.804 \tLoss: 0.9281\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 54.551 \tLoss: 1.3023\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  10, Avg Accuracy 71.623 | Avg Loss 0.887\n",
      " Test: Round  10, Avg Accuracy 49.604 | Avg Loss 2.010\n",
      "==========================================================\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 73.802 \tLoss: 0.7935\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 51.611 \tLoss: 1.6824\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 74.520 \tLoss: 0.8156\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 45.420 \tLoss: 1.9163\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 68.283 \tLoss: 0.9652\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 51.924 \tLoss: 1.6777\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 70.827 \tLoss: 0.8704\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 52.812 \tLoss: 1.3324\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 77.384 \tLoss: 0.6780\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 43.877 \tLoss: 2.2610\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  11, Avg Accuracy 72.963 | Avg Loss 0.825\n",
      " Test: Round  11, Avg Accuracy 49.129 | Avg Loss 1.774\n",
      "==========================================================\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 80.030 \tLoss: 0.5580\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 43.652 \tLoss: 3.3530\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 65.285 \tLoss: 1.1826\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 52.773 \tLoss: 1.3422\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 74.796 \tLoss: 0.7832\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 49.805 \tLoss: 1.8140\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 73.614 \tLoss: 0.8317\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 48.379 \tLoss: 1.7474\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 70.662 \tLoss: 0.8849\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 51.973 \tLoss: 1.6475\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  12, Avg Accuracy 72.877 | Avg Loss 0.848\n",
      " Test: Round  12, Avg Accuracy 49.316 | Avg Loss 1.981\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.132 \tLoss: 0.8399\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 54.150 \tLoss: 1.3714\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 75.615 \tLoss: 0.7763\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 49.170 \tLoss: 1.7418\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 69.164 \tLoss: 0.9119\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 56.904 \tLoss: 1.5583\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 75.060 \tLoss: 0.7623\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 55.156 \tLoss: 1.4781\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 78.546 \tLoss: 0.6411\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 43.926 \tLoss: 2.3063\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  13, Avg Accuracy 74.103 | Avg Loss 0.786\n",
      " Test: Round  13, Avg Accuracy 51.861 | Avg Loss 1.691\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 69.007 \tLoss: 0.9835\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 54.199 \tLoss: 1.2763\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 79.231 \tLoss: 0.5924\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 44.795 \tLoss: 2.6760\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 70.346 \tLoss: 0.8885\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 54.150 \tLoss: 1.5763\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 73.313 \tLoss: 0.8968\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 49.736 \tLoss: 1.7446\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 75.625 \tLoss: 0.7595\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 54.336 \tLoss: 1.6516\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  14, Avg Accuracy 73.504 | Avg Loss 0.824\n",
      " Test: Round  14, Avg Accuracy 51.443 | Avg Loss 1.785\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.869 \tLoss: 0.8335\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 55.137 \tLoss: 1.2693\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 79.688 \tLoss: 0.5933\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.957 \tLoss: 2.5616\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 74.997 \tLoss: 0.7930\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 47.158 \tLoss: 1.9087\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 75.760 \tLoss: 0.7552\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 51.348 \tLoss: 1.8793\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 72.425 \tLoss: 0.8390\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 56.992 \tLoss: 1.4744\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  15, Avg Accuracy 75.147 | Avg Loss 0.763\n",
      " Test: Round  15, Avg Accuracy 51.318 | Avg Loss 1.819\n",
      "==========================================================\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 77.511 \tLoss: 0.6736\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 56.475 \tLoss: 1.7946\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 75.196 \tLoss: 0.8515\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 49.121 \tLoss: 1.8159\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.474 \tLoss: 0.8739\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 57.598 \tLoss: 1.1994\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 72.247 \tLoss: 0.8000\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 58.750 \tLoss: 1.5746\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 79.982 \tLoss: 0.6080\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.791 \tLoss: 2.2634\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  16, Avg Accuracy 75.482 | Avg Loss 0.761\n",
      " Test: Round  16, Avg Accuracy 53.547 | Avg Loss 1.730\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 70.654 \tLoss: 0.9303\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 57.627 \tLoss: 1.1903\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 76.746 \tLoss: 0.7566\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 53.828 \tLoss: 1.7349\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 76.802 \tLoss: 0.7238\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 53.594 \tLoss: 1.6268\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 73.956 \tLoss: 0.8483\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 55.664 \tLoss: 1.6193\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 80.565 \tLoss: 0.5869\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 46.270 \tLoss: 2.2788\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  17, Avg Accuracy 75.745 | Avg Loss 0.769\n",
      " Test: Round  17, Avg Accuracy 53.396 | Avg Loss 1.690\n",
      "==========================================================\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 71.457 \tLoss: 0.9414\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 58.965 \tLoss: 1.1862\u001b[00m\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 77.551 \tLoss: 0.7316\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 52.979 \tLoss: 1.6902\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 77.335 \tLoss: 0.7024\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 56.309 \tLoss: 1.6611\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 73.767 \tLoss: 0.7868\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 58.965 \tLoss: 1.4353\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 81.468 \tLoss: 0.5610\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 45.557 \tLoss: 2.5863\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  18, Avg Accuracy 76.316 | Avg Loss 0.745\n",
      " Test: Round  18, Avg Accuracy 54.555 | Avg Loss 1.712\n",
      "==========================================================\n",
      "\u001b[91m Client4 Train => Local Epoch: 0 \tAcc: 76.197 \tLoss: 0.8066\u001b[00m\n",
      "\u001b[92m Client4 Test =>                   \tAcc: 48.730 \tLoss: 1.9478\u001b[00m\n",
      "\u001b[91m Client0 Train => Local Epoch: 0 \tAcc: 72.839 \tLoss: 0.8977\u001b[00m\n",
      "\u001b[92m Client0 Test =>                   \tAcc: 59.531 \tLoss: 1.2746\u001b[00m\n",
      "\u001b[91m Client2 Train => Local Epoch: 0 \tAcc: 76.689 \tLoss: 0.7118\u001b[00m\n",
      "\u001b[92m Client2 Test =>                   \tAcc: 59.766 \tLoss: 1.5476\u001b[00m\n",
      "\u001b[91m Client3 Train => Local Epoch: 0 \tAcc: 80.882 \tLoss: 0.5625\u001b[00m\n",
      "\u001b[92m Client3 Test =>                   \tAcc: 46.406 \tLoss: 2.4755\u001b[00m\n",
      "\u001b[91m Client1 Train => Local Epoch: 0 \tAcc: 72.837 \tLoss: 0.8381\u001b[00m\n",
      "\u001b[92m Client1 Test =>                   \tAcc: 55.156 \tLoss: 1.6250\u001b[00m\n",
      "====================== SERVER V1==========================\n",
      " Train: Round  19, Avg Accuracy 75.889 | Avg Loss 0.763\n",
      " Test: Round  19, Avg Accuracy 53.918 | Avg Loss 1.774\n",
      "==========================================================\n",
      "Training and Evaluation completed!\n",
      "Saved: SL ResNet18 on CIFAR-10 - with Projection.xlsx and SL ResNet18 on CIFAR-10 - with Projection.csv\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================\n",
    "# Split learning: ResNet18 on HAM10000\n",
    "# HAM10000 dataset: Tschandl, P.: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions (2018), doi:10.7910/DVN/DBW86T\n",
    "\n",
    "# We have three versions of our implementations\n",
    "# Version1: without using socket and no DP+PixelDP\n",
    "# Version2: with using socket but no DP+PixelDP\n",
    "# Version3: without using socket but with DP+PixelDP\n",
    "\n",
    "# This program is Version1: Single program simulation \n",
    "# ============================================================================\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import math, time\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from glob import glob \n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# ORTHOGONAL\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def _sync_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Orthogonal Config\n",
    "#=====================================================================================================\n",
    "# Orthogonal projection config deque\n",
    "ORTH = True\n",
    "\n",
    "# number of principal directions to set for \"old subspace\"\n",
    "TOP_R = 32\n",
    "\n",
    "# total samples for building U_old\n",
    "FEATURE_MEM_CAP = 2048\n",
    "FEATURE_MEM = deque(maxlen=FEATURE_MEM_CAP)\n",
    "\n",
    "# holding D x r matrix of principal directions at the cut\n",
    "U_old = None\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Projection Help\n",
    "#=====================================================================================================\n",
    "def project_off_subspace(grad_batch: torch.Tensor, U: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Might be????\n",
    "    grad_batch: gradient wrt cut activations, shape [B, C, H, W] (or [B, D...]).\n",
    "    U:          [D, r] orthonormal basis of \"old\" subspace (columns)\n",
    "    Returns:    projected gradient with same shape as grad_batch\n",
    "    \"\"\"\n",
    "    if U is None:\n",
    "        return grad_batch\n",
    "    B = grad_batch.shape[0]\n",
    "    g = grad_batch.view(B, -1)  # [B, D]\n",
    "    # projection onto U: (g U) U^T, then subtract\n",
    "    g_proj = g - (g @ U) @ U.T  # [B, D]\n",
    "    return g_proj.view_as(grad_batch)\n",
    "\n",
    "\n",
    "#=====================================================================================================\n",
    "#         Subspace builder (simple PCA/SVD on buffered features)\n",
    "#=====================================================================================================\n",
    "def build_U_old_from_memory(top_r=TOP_R, device='cpu'):\n",
    "    \"\"\"\n",
    "    Uses FEATURE_MEM (of flattened cut activations) to build U_old via SVD.\n",
    "    Returns U_old of shape [D, r] with orthonormal columns (on 'device').\n",
    "    \"\"\"\n",
    "    if len(FEATURE_MEM) == 0:\n",
    "        return None\n",
    "    with torch.no_grad():\n",
    "        X = torch.cat(list(FEATURE_MEM), dim=0)   # [N, D]\n",
    "        # center\n",
    "        X = X - X.mean(dim=0, keepdim=True)\n",
    "        # economy SVD on CPU to avoid GPU OOMs for big D/N; then move back\n",
    "        X_cpu = X.cpu()\n",
    "        # torch.linalg.svd: X = U Î£ Vh; principal directions in Vh^T (columns of V)\n",
    "        U_, S_, Vh_ = torch.linalg.svd(X_cpu, full_matrices=False)\n",
    "        V_ = Vh_.T[:, :min(top_r, Vh_.shape[0])]  # [D, r]\n",
    "        U_mat = V_.to(device)\n",
    "        # (Optional) re-orthonormalize (usually already orthonormal from SVD)\n",
    "        return U_mat\n",
    "\n",
    "#=====================================================================================================\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(torch.cuda.get_device_name(0))    \n",
    "\n",
    "#===================================================================  \n",
    "program = \"SL ResNet18 on CIFAR-10 - with Projection\"\n",
    "print(f\"---------{program}----------\")              # this is to identify the program in the slurm outputs files\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# To print in color -------test/train of the client side\n",
    "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk)) \n",
    "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))     \n",
    "\n",
    "#===================================================================  \n",
    "# No. of users\n",
    "num_users = 5\n",
    "epochs = 20\n",
    "frac = 1   # participation of clients; if 1 then 100% clients participate in SL\n",
    "lr = 0.0001\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Client-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at client side\n",
    "class ResNet18_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18_client_side, self).__init__()\n",
    "        self.layer1 = nn.Sequential (\n",
    "                nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU (inplace = True),\n",
    "                nn.MaxPool2d(kernel_size = 3, stride = 2, padding =1),\n",
    "            )\n",
    "        self.layer2 = nn.Sequential  (\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU (inplace = True),\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.BatchNorm2d(64),              \n",
    "            )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        resudial1 = F.relu(self.layer1(x))\n",
    "        out1 = self.layer2(resudial1)\n",
    "        out1 = out1 + resudial1 # adding the resudial inputs -- downsampling not required in this layer\n",
    "        resudial2 = F.relu(out1)\n",
    "        return resudial2\n",
    " \n",
    " \n",
    "           \n",
    "\n",
    "net_glob_client = ResNet18_client_side()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_client = nn.DataParallel(net_glob_client)   # to use the multiple GPUs; later we can change this to CPUs only \n",
    "\n",
    "net_glob_client.to(device)\n",
    "print(net_glob_client)     \n",
    "\n",
    "\n",
    "#=====================================================================================================\n",
    "#                           Server-side Model definition\n",
    "#=====================================================================================================\n",
    "# Model at server side\n",
    "class Baseblock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, input_planes, planes, stride = 1, dim_change = None):\n",
    "        super(Baseblock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_planes, planes, stride =  stride, kernel_size = 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, stride = 1, kernel_size = 3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        \n",
    "        if self.dim_change is not None:\n",
    "            res =self.dim_change(res)\n",
    "            \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ResNet18_server_side(nn.Module):\n",
    "    def __init__(self, block, num_layers, classes):\n",
    "        super(ResNet18_server_side, self).__init__()\n",
    "        self.input_planes = 64\n",
    "        self.layer3 = nn.Sequential (\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU (inplace = True),\n",
    "                nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.BatchNorm2d(64),       \n",
    "                )   \n",
    "        \n",
    "        self.layer4 = self._layer(block, 128, num_layers[0], stride = 2)\n",
    "        self.layer5 = self._layer(block, 256, num_layers[1], stride = 2)\n",
    "        self.layer6 = self._layer(block, 512, num_layers[2], stride = 2)\n",
    "        self. averagePool = nn.AvgPool2d(kernel_size = 7, stride = 1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "    def _layer(self, block, planes, num_layers, stride = 2):\n",
    "        dim_change = None\n",
    "        if stride != 1 or planes != self.input_planes * block.expansion:\n",
    "            dim_change = nn.Sequential(nn.Conv2d(self.input_planes, planes*block.expansion, kernel_size = 1, stride = stride),\n",
    "                                       nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers = []\n",
    "        netLayers.append(block(self.input_planes, planes, stride = stride, dim_change = dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1, num_layers):\n",
    "            netLayers.append(block(self.input_planes, planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "            \n",
    "        return nn.Sequential(*netLayers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out2 = self.layer3(x)\n",
    "        out2 = out2 + x          # adding the resudial inputs -- downsampling not required in this layer\n",
    "        x3 = F.relu(out2)\n",
    "        \n",
    "        x4 = self. layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "        x6 = self.layer6(x5)\n",
    "        \n",
    "        # x7 = F.avg_pool2d(x6, 7)\n",
    "        x7 = F.adaptive_avg_pool2d(x6, 1) \n",
    "        x8 = x7.view(x7.size(0), -1) \n",
    "        y_hat =self.fc(x8)\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "# net_glob_server = ResNet18_server_side(Baseblock, [2,2,2], 7) #7 is my numbr of classes\n",
    "net_glob_server = ResNet18_server_side(Baseblock, [2,2,2], 10)  # 10 classes for CIFAR-10\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\",torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_server = nn.DataParallel(net_glob_server)   # to use the multiple GPUs \n",
    "\n",
    "net_glob_server.to(device)\n",
    "print(net_glob_server)      \n",
    "\n",
    "#===================================================================================\n",
    "# For Server Side Loss and Accuracy \n",
    "loss_train_collect = []\n",
    "acc_train_collect = []\n",
    "loss_test_collect = []\n",
    "acc_test_collect = []\n",
    "batch_acc_train = []\n",
    "batch_loss_train = []\n",
    "batch_acc_test = []\n",
    "batch_loss_test = []\n",
    "round_time_collect = [] # Stores seconds per global round\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "#====================================================================================================\n",
    "#                                  Server Side Program\n",
    "#====================================================================================================\n",
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = 100.00 *correct.float()/preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "# to print train - test together in each round-- these are made global\n",
    "acc_avg_all_user_train = 0\n",
    "loss_avg_all_user_train = 0\n",
    "loss_train_collect_user = []\n",
    "acc_train_collect_user = []\n",
    "loss_test_collect_user = []\n",
    "acc_test_collect_user = []\n",
    "\n",
    "\n",
    "#client idx collector\n",
    "idx_collect = []\n",
    "l_epoch_check = False\n",
    "fed_check = False\n",
    "\n",
    "# Server-side function associated with Training \n",
    "def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch):\n",
    "    global net_glob_server, criterion, device, batch_acc_train, batch_loss_train, l_epoch_check, fed_check\n",
    "    global loss_train_collect, acc_train_collect, count1, acc_avg_all_user_train, loss_avg_all_user_train, idx_collect\n",
    "    global loss_train_collect_user, acc_train_collect_user\n",
    "    \n",
    "    net_glob_server.train()\n",
    "    optimizer_server = torch.optim.Adam(net_glob_server.parameters(), lr = lr)\n",
    "\n",
    "    \n",
    "    # train and update\n",
    "    optimizer_server.zero_grad()\n",
    "    \n",
    "    fx_client = fx_client.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    #---------forward prop-------------\n",
    "    fx_server = net_glob_server(fx_client)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(fx_server, y)\n",
    "    # calculate accuracy\n",
    "    acc = calculate_accuracy(fx_server, y)\n",
    "    \n",
    "    #--------backward prop--------------\n",
    "    # loss.backward()\n",
    "    # dfx_client = fx_client.grad.clone().detach()\n",
    "    # optimizer_server.step()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # -------------- ORTH PROJECTION AT THE CUT GRADIENT --------------\n",
    "    dfx_client = fx_client.grad.clone().detach()\n",
    "    if ORTH and (U_old is not None):\n",
    "        dfx_client = project_off_subspace(dfx_client, U_old)\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    optimizer_server.step()\n",
    "\n",
    "    \n",
    "    batch_loss_train.append(loss.item())\n",
    "    batch_acc_train.append(acc.item())\n",
    "    \n",
    "    # server-side model net_glob_server is global so it is updated automatically in each pass to this function\n",
    "        # count1: to track the completion of the local batch associated with one client\n",
    "    count1 += 1\n",
    "    if count1 == len_batch:\n",
    "        acc_avg_train = sum(batch_acc_train)/len(batch_acc_train)           # it has accuracy for one batch\n",
    "        loss_avg_train = sum(batch_loss_train)/len(batch_loss_train)\n",
    "        \n",
    "        batch_acc_train = []\n",
    "        batch_loss_train = []\n",
    "        count1 = 0\n",
    "        \n",
    "        prRed('Client{} Train => Local Epoch: {} \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, l_epoch_count, acc_avg_train, loss_avg_train))\n",
    "        \n",
    "                \n",
    "        # If one local epoch is completed, after this a new client will come\n",
    "        if l_epoch_count == l_epoch-1:\n",
    "            \n",
    "            l_epoch_check = True                # for evaluate_server function - to check local epoch has hitted \n",
    "                       \n",
    "            # we store the last accuracy in the last batch of the epoch and it is not the average of all local epochs\n",
    "            # this is because we work on the last trained model and its accuracy (not earlier cases)\n",
    "            \n",
    "            #print(\"accuracy = \", acc_avg_train)\n",
    "            acc_avg_train_all = acc_avg_train\n",
    "            loss_avg_train_all = loss_avg_train\n",
    "                        \n",
    "            # accumulate accuracy and loss for each new user\n",
    "            loss_train_collect_user.append(loss_avg_train_all)\n",
    "            acc_train_collect_user.append(acc_avg_train_all)\n",
    "            \n",
    "            # collect the id of each new user                        \n",
    "            if idx not in idx_collect:\n",
    "                idx_collect.append(idx) \n",
    "                #print(idx_collect)\n",
    "        \n",
    "        # This is to check if all users are served for one round --------------------\n",
    "        if len(idx_collect) == num_users:\n",
    "            fed_check = True                                                  # for evaluate_server function  - to check fed check has hitted\n",
    "            # all users served for one round ------------------------- output print and update is done in evaluate_server()\n",
    "            # for nicer display \n",
    "                        \n",
    "            idx_collect = []\n",
    "            \n",
    "            acc_avg_all_user_train = sum(acc_train_collect_user)/len(acc_train_collect_user)\n",
    "            loss_avg_all_user_train = sum(loss_train_collect_user)/len(loss_train_collect_user)\n",
    "            \n",
    "            loss_train_collect.append(loss_avg_all_user_train)\n",
    "            acc_train_collect.append(acc_avg_all_user_train)\n",
    "            \n",
    "            acc_train_collect_user = []\n",
    "            loss_train_collect_user = []\n",
    "            \n",
    "    # send gradients to the client               \n",
    "    return dfx_client\n",
    "\n",
    "# Server-side functions associated with Testing\n",
    "def evaluate_server(fx_client, y, idx, len_batch, ell):\n",
    "    global net_glob_server, criterion, batch_acc_test, batch_loss_test\n",
    "    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, l_epoch_check, fed_check\n",
    "    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n",
    "    \n",
    "    net_glob_server.eval()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        fx_client = fx_client.to(device)\n",
    "        y = y.to(device) \n",
    "        #---------forward prop-------------\n",
    "        fx_server = net_glob_server(fx_client)\n",
    "        \n",
    "        # ---- collect cut activations (features at the cut) for building U_old ----\n",
    "        if ORTH:\n",
    "            with torch.no_grad():\n",
    "                feats = fx_client.detach().view(fx_client.shape[0], -1)  # [B, D]\n",
    "                FEATURE_MEM.append(feats.cpu())  # store on CPU to save GPU memory\n",
    "\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(fx_server, y)\n",
    "        # calculate accuracy\n",
    "        acc = calculate_accuracy(fx_server, y)\n",
    "        \n",
    "        \n",
    "        batch_loss_test.append(loss.item())\n",
    "        batch_acc_test.append(acc.item())\n",
    "        \n",
    "               \n",
    "        count2 += 1\n",
    "        if count2 == len_batch:\n",
    "            acc_avg_test = sum(batch_acc_test)/len(batch_acc_test)\n",
    "            loss_avg_test = sum(batch_loss_test)/len(batch_loss_test)\n",
    "            \n",
    "            batch_acc_test = []\n",
    "            batch_loss_test = []\n",
    "            count2 = 0\n",
    "            \n",
    "            prGreen('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f}'.format(idx, acc_avg_test, loss_avg_test))\n",
    "            \n",
    "            # if a local epoch is completed   \n",
    "            if l_epoch_check:\n",
    "                l_epoch_check = False\n",
    "                \n",
    "                # Store the last accuracy and loss\n",
    "                acc_avg_test_all = acc_avg_test\n",
    "                loss_avg_test_all = loss_avg_test\n",
    "                        \n",
    "                loss_test_collect_user.append(loss_avg_test_all)\n",
    "                acc_test_collect_user.append(acc_avg_test_all)\n",
    "                \n",
    "            # if all users are served for one round ----------                    \n",
    "            if fed_check:\n",
    "                fed_check = False\n",
    "                                \n",
    "                acc_avg_all_user = sum(acc_test_collect_user)/len(acc_test_collect_user)\n",
    "                loss_avg_all_user = sum(loss_test_collect_user)/len(loss_test_collect_user)\n",
    "            \n",
    "                loss_test_collect.append(loss_avg_all_user)\n",
    "                acc_test_collect.append(acc_avg_all_user)\n",
    "                acc_test_collect_user = []\n",
    "                loss_test_collect_user= []\n",
    "                              \n",
    "                print(\"====================== SERVER V1==========================\")\n",
    "                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train, loss_avg_all_user_train))\n",
    "                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user, loss_avg_all_user))\n",
    "                print(\"==========================================================\")\n",
    "         \n",
    "            # ---- (Re)build U_old at round boundaries ----\n",
    "            global U_old\n",
    "            if ORTH:\n",
    "                U_old = build_U_old_from_memory(top_r=TOP_R, device=device)\n",
    "    \n",
    "    return \n",
    "\n",
    "#==============================================================================================================\n",
    "#                                       Clients Side Program\n",
    "#==============================================================================================================\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "# Client-side functions associated with Training and Testing\n",
    "class Client(object):\n",
    "    def __init__(self, net_client_model, idx, lr, device, dataset_train = None, dataset_test = None, idxs = None, idxs_test = None):\n",
    "        self.idx = idx\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.local_ep = 1 \n",
    "        #self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset_train, idxs), batch_size = 128, shuffle = True)\n",
    "        self.ldr_test = DataLoader(DatasetSplit(dataset_test, idxs_test), batch_size = 128, shuffle = True)\n",
    "        \n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr = self.lr) \n",
    "        \n",
    "        for iter in range(self.local_ep):\n",
    "            len_batch = len(self.ldr_train)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer_client.zero_grad()\n",
    "                #---------forward prop-------------\n",
    "                fx = net(images)\n",
    "                client_fx = fx.clone().detach().requires_grad_(True)\n",
    "                \n",
    "                # Sending activations to server and receiving gradients from server\n",
    "                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n",
    "                \n",
    "                #--------backward prop -------------\n",
    "                fx.backward(dfx)\n",
    "                optimizer_client.step()\n",
    "                            \n",
    "            \n",
    "            #prRed('Client{} Train => Epoch: {}'.format(self.idx, ell))\n",
    "           \n",
    "        return net.state_dict() \n",
    "    \n",
    "    def evaluate(self, net, ell):\n",
    "        net.eval()\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            len_batch = len(self.ldr_test)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                #---------forward prop-------------\n",
    "                fx = net(images)\n",
    "                \n",
    "                # Sending activations to server \n",
    "                evaluate_server(fx, labels, self.idx, len_batch, ell)\n",
    "            \n",
    "            #prRed('Client{} Test => Epoch: {}'.format(self.idx, ell))\n",
    "            \n",
    "        return          \n",
    "#=====================================================================================================\n",
    "# dataset_iid() will create a dictionary to collect the indices of the data samples randomly for each client\n",
    "# IID HAM10000 datasets will be created based on this\n",
    "def dataset_iid(dataset, num_users):\n",
    "    \n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace = False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users    \n",
    "\n",
    "def dataset_noniid_dirichlet(dataset, num_users, alpha=0.5, seed=SEED):\n",
    "    \"\"\"Return dict_users: {client_id -> set(sample_indices)} using Dirichlet(Î±).\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    labels = np.array(dataset.targets)  # CIFAR-10 stores labels here\n",
    "    n_classes = len(np.unique(labels))\n",
    "    dict_users = {i: set() for i in range(num_users)}\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        rng.shuffle(idxs)\n",
    "        # draw proportions for this class\n",
    "        p = rng.dirichlet([alpha] * num_users)\n",
    "        # split indices by cumulative proportions\n",
    "        cuts = (np.cumsum(p) * len(idxs)).astype(int)[:-1]\n",
    "        splits = np.split(idxs, cuts)\n",
    "        for client_id, split in enumerate(splits):\n",
    "            dict_users[client_id].update(split.tolist())\n",
    "    return dict_users\n",
    "\n",
    "            \n",
    "#=============================================================================\n",
    "#                         Data loading \n",
    "#============================================================================= \n",
    "# df = pd.read_csv('data/HAM10000_metadata.csv')\n",
    "# print(df.head())\n",
    "\n",
    "# lesion_type = {\n",
    "#     'nv': 'Melanocytic nevi',\n",
    "#     'mel': 'Melanoma',\n",
    "#     'bkl': 'Benign keratosis-like lesions ',\n",
    "#     'bcc': 'Basal cell carcinoma',\n",
    "#     'akiec': 'Actinic keratoses',\n",
    "#     'vasc': 'Vascular lesions',\n",
    "#     'df': 'Dermatofibroma'\n",
    "# }\n",
    "\n",
    "# # merging both folders of HAM1000 dataset -- part1 and part2 -- into a single directory\n",
    "# imageid_path = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "#                 for x in glob(os.path.join(\"data\", '*', '*.jpg'))}\n",
    "\n",
    "\n",
    "# #print(\"path---------------------------------------\", imageid_path.get)\n",
    "# df['path'] = df['image_id'].map(imageid_path.get)\n",
    "# df['cell_type'] = df['dx'].map(lesion_type.get)\n",
    "# df['target'] = pd.Categorical(df['cell_type']).codes\n",
    "# print(df['cell_type'].value_counts())\n",
    "# print(df['target'].value_counts())\n",
    "\n",
    "\n",
    "# # (optional) see how many are missing\n",
    "# print(\"Missing images:\", df['path'].isna().sum())\n",
    "\n",
    "# # drop rows with missing/invalid paths\n",
    "# df = df.dropna(subset=['path']).reset_index(drop=True)\n",
    "\n",
    "#==============================================================\n",
    "# Custom dataset prepration in Pytorch format\n",
    "# class SkinData(Dataset):\n",
    "#     def __init__(self, df, transform = None):\n",
    "#         self.df = df\n",
    "#         self.transform = transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "       \n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "                \n",
    "#         # X = Image.open(self.df['path'][index]).resize((64, 64))\n",
    "#         X = Image.open(self.df['path'][index]).convert('RGB').resize((64, 64))\n",
    "\n",
    "#         y = torch.tensor(int(self.df['target'][index]))\n",
    "        \n",
    "#         if self.transform:\n",
    "#             X = self.transform(X)\n",
    "        \n",
    "#         return X, y\n",
    "# #=============================================================================\n",
    "# # Train-test split      \n",
    "# train, test = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "# train = train.reset_index()\n",
    "# test = test.reset_index()\n",
    "#=============================================================================\n",
    "#                         Data preprocessing\n",
    "#=============================================================================  \n",
    "# Data preprocessing: Transformation \n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "#                         transforms.RandomVerticalFlip(),\n",
    "#                         transforms.Pad(3),\n",
    "#                         transforms.RandomRotation(10),\n",
    "#                         transforms.CenterCrop(64),\n",
    "#                         transforms.ToTensor(), \n",
    "#                         transforms.Normalize(mean = mean, std = std)\n",
    "#                         ])\n",
    "    \n",
    "# test_transforms = transforms.Compose([\n",
    "#                         transforms.Pad(3),\n",
    "#                         transforms.CenterCrop(64),\n",
    "#                         transforms.ToTensor(), \n",
    "#                         transforms.Normalize(mean = mean, std = std)\n",
    "#                         ])    \n",
    "\n",
    "\n",
    "# # With augmentation\n",
    "# dataset_train = SkinData(train, transform = train_transforms)\n",
    "# dataset_test = SkinData(test, transform = test_transforms)\n",
    "\n",
    "\n",
    "# CIFAR-10 normalization\n",
    "cifar_mean = [0.4914, 0.4822, 0.4465]\n",
    "cifar_std  = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
    "])\n",
    "\n",
    "dataset_train = datasets.CIFAR10(root=\"data\", train=True,  download=True, transform=train_transforms)\n",
    "dataset_test  = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transforms)\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# dict_users = dataset_iid(dataset_train, num_users)\n",
    "# dict_users_test = dataset_iid(dataset_test, num_users)\n",
    "\n",
    "ALPHA = 0.5   # â your requested alpha\n",
    "\n",
    "dict_users      = dataset_noniid_dirichlet(dataset_train, num_users, alpha=ALPHA)\n",
    "dict_users_test = dataset_iid(dataset_test, num_users)  \n",
    "\n",
    "\n",
    "\n",
    "#net_glob_client.train()\n",
    "# this epoch is global epoch, also known as rounds\n",
    "# this epoch is global epoch, also known as rounds\n",
    "for iter in range(epochs):\n",
    "    # ---- START TIMER ----\n",
    "    _sync_cuda()\n",
    "    t0 = time.perf_counter()\n",
    "    # ---------------------\n",
    "\n",
    "    m = max(int(frac * num_users), 1)\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace=False)\n",
    "\n",
    "    # Sequential training/testing among clients      \n",
    "    for idx in idxs_users:\n",
    "        local = Client(net_glob_client, idx, lr, device,\n",
    "                       dataset_train=dataset_train, dataset_test=dataset_test,\n",
    "                       idxs=dict_users[idx], idxs_test=dict_users_test[idx])\n",
    "\n",
    "        # Training\n",
    "        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n",
    "              \n",
    "        # Testing\n",
    "        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter)\n",
    "        \n",
    "        # Update client weights for next client\n",
    "        net_glob_client.load_state_dict(w_client)\n",
    "\n",
    "    # ---- STOP TIMER & LOG ----\n",
    "    _sync_cuda()\n",
    "    dt = time.perf_counter() - t0\n",
    "    round_time_collect.append(dt)\n",
    "    # --------------------------\n",
    "  \n",
    "\n",
    "print(\"Training and Evaluation completed!\")    \n",
    "\n",
    "#===============================================================================\n",
    "# Save output data to .excel file (we use for comparision plots)\n",
    "#================ Save outputs ================#\n",
    "# Align lengths safely (in case of any mismatch)\n",
    "n = min(len(acc_train_collect), len(acc_test_collect), len(round_time_collect))\n",
    "round_process = list(range(1, n+1))\n",
    "\n",
    "df = DataFrame({\n",
    "    'round': round_process,\n",
    "    'acc_train': acc_train_collect[:n],\n",
    "    'acc_test':  acc_test_collect[:n],\n",
    "    'round_time_sec': round_time_collect[:n],   # NEW: time per round\n",
    "    # Helpful tags so the compare notebook can label/filter nicely\n",
    "    'projection': [True]*n,\n",
    "    'top_r': [TOP_R]*n,\n",
    "    'feature_mem_cap': [FEATURE_MEM_CAP]*n,\n",
    "    'alpha': [ALPHA]*n,\n",
    "    'num_users': [num_users]*n,\n",
    "    'lr': [lr]*n,\n",
    "    'seed': [SEED]*n,\n",
    "})\n",
    "\n",
    "file_name_xlsx = program + \".xlsx\"\n",
    "df.to_excel(file_name_xlsx, sheet_name=\"v1_test\", index=False)\n",
    "\n",
    "# Optional CSV (makes the compare notebook and scripting easier)\n",
    "file_name_csv = program + \".csv\"\n",
    "df.to_csv(file_name_csv, index=False)\n",
    "\n",
    "print(\"Saved:\", file_name_xlsx, \"and\", file_name_csv)\n",
    "\n",
    "\n",
    "#=============================================================================\n",
    "#                         Program Completed\n",
    "#============================================================================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8c5b5-fbc2-4e74-8f5d-4ffffb39f044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
